
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the TDPTemplate using
% the LaTeX document class 'llncs.cls' Springer LNAI format
% used in the RoboCup Symposium submissions.
% http://www.springer.com/computer/lncs?SGWID=0-164-6-793341-0
%
% It may be used as a template for your own TDP - copy it
% to a new file with a new name and use it as the basis
% for your Team Description Paper
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[runningheads,a4paper]{llncs}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{tabto}
\usepackage{lipsum}
\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref}

\begin{document}


\newif\ifdraft
\draftfalse


\ifdraft
\setlength{\belowcaptionskip}{-5pt}
\fi

\title{Walking Machine @Home \newline \: 2018 Team Description Paper}

\author{Jeffrey Cousineau and Philippe La Madeleine}
\institute{École de Technologie Supérieure \\ 1100 rue Notre-Dame Ouest, Montreal, QC, Canada H3C 1K3 \\
\texttt{http://walkingmachine.ca,} \texttt{walking@ens.etsmtl.ca,} \texttt{https://github.com/WalkingMachine}}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

This paper gives details about the RoboCup@Home league team Walking Machine, from ETS University in Montreal, Canada for the next competition in his hometown, Montreal, in July 2018. The robot from Walking Machine, named SARA for "Système d’Assistance Robotique Autonome" (in English, Automated Robotic Assistance System), is a robot entirely built by the scientific club from ETS, mainly composed of undergraduates students. The robot is used for social interaction with humans, navigation and object manipulation. This document shows the electrical, mechanical and software novelties and functionalities of SARA.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\tab Walking Machine’s team is a young team from Montreal, Quebec, in Canada, composed of engineering student in the field of mechanical, electrical and software engineering. We have been working really hard to improve our robot for the next year Robocup@Home competition. As this would be our third participation, we learned a lot on our second attempt last year and have made many improvements to get better results, mostly on the software side. In the past, the team went in many competitions like the Eurobot, but made the leap for the RoboCup@Home competition to get a bigger challenge and to get an opportunity to bring novelty in the scientific community around robotic. \\

%TODO : Modify this paragraph
SARA, our creation, was designed for polyvalent human-robot interaction as well as efficient navigation and object manipulation. Our robot is mounted on four mecanum wheels powered by Roboteq drives, has one arm mimicking a normal human arm, and sensors for communication and navigation. Our team has developed knowledge in object and people detection/recognition, as well as navigation using a laser scanner, odometry on the wheels and a Asus Xtion camera. All of these parts are interfaced through ROS (Robot Operating System). \\

\section{Electrical and mechanical improvement}
\subsection{Electrical}

\tab As an improvement this year on the electrical side of our robot, we put a lot of efforts in the organization of the electrical systems, we made it much clearer and safer. We put a lot of protection for every sub-system since it’s easier to just change a fuse than rebuilding a whole electronic circuit board.\\

Another big change for us was the battery system. Just before leaving for Japan this year, we changed from our custom LiPo battery to a system using common tool batteries. This change has bringed way more positive aspects than we expected. First of all, it gives us the ability to ship the robot batteryless and to bring the batteries on our personal luggages. Which is a big deal when it come to shipping robots overseas.\\

Second impact is that this type of battery is way safer and convenient than what we had before. We can simply use the commercial charging station to charge our batteries, that way we don’t have to worry about overcharges or fires. Ours onboard dual batteries layout also bring enough charge to power our robot for a decent amount of time and allow us to easily perform live battery swapping without having to shut down the robot.\\



\subsection{Mechanical}

\tab Some mechanical improvement were also made this year. There’s mainly two things we improved on our robot, the arm and the base.\\

First of all, last year, our arm was made of 5 degrees of freedom, which makes some path plan a little more complicated and sometimes, impossible. Because of that, we decide to add 2 degrees of freedom with 2 dynamixel servo motors. Since then, we have much better results with our inverse kinematic and this also extend our range to get objects that are further.\\
\begin{wrapfigure}[10]{r}{0.20\textwidth}
	\centering
	\includegraphics[width=0.10\textwidth]{images/arm.png}
	\caption{SARA 7 DoF arm}
\end{wrapfigure}

Next thing we improve was the compactness of our robot base. On our first competition in Germany, we observe that our robot was too large, which caused some problems with our path planning around the doors. By relying on these observations, we decided to reduce the width which improved a lot the navigation capabilities. \\
\newpage
\section{Software}

\subsection{High-level task planning}
\tab For our task planning, we use a state machine software developed by team Vigir, one of the participant of the Darpa Robotics Challenge. This software, named \href{http://philserver.bplaced.net/fbe/index.php}{FlexBe}, for Flexible Behavior, is a block based interface for making state machines.\\



\begin{figure}[h!]
	\centering
	\includegraphics[width=0.70\textwidth]{images/flexbe.png}
	\caption{FlexBe}
\end{figure}

But we do not simply use FlexBe as is. We still need to write our own states using its python API. To simplify our work, we started by splitting all of the Robocup@home scenarios into basic actions. We then identified all of these basic actions our platform could accomplish and we confined them in blocks named States, e.g. MoveArm, MoveHead etc. These blocks can then be assembled together to form a higher level of blocks we named Actions e.g. Pick, LookAt etc. Those Actions can then again be assembled into what we call ActionWrappers. The role of ActionWrapper is to allow interfacing our Actions with our natural language processing software(see natural language processing below). They receive the segmented text and translate it to computer understandable parameters using our Wonderland (see environment reasoning below) knowledge base and other sources of informations. This recursive structure allow us to quickly develop our robot’s behaviours.\\

\subsection{Natural language processing}
\tab To analyse the detected speech, we rely on a software develop by the  \href{http://sag.art.uniroma2.it/}{Semantic analytic group from the University of Roma} and the \href{http://labrococo.dis.uniroma1.it/}{Laboratory of Cognitive Cooperating Robots} at Sapienza University of Rome. This speech analyzer, named \href{http://sag.art.uniroma2.it/lu4r.html}{LU4R} for “Language Understanding For Robots”, is composed of a server developed in Java which take as an input the detected sentence and the semantic environment surrounding the robot.\\

This server communicate through a REST service which give it the possibility to be compliant with all kind of platform. All you have to do is to launch the server locally which is compiled through a .jar file. Again, this give the opportunity to use this software on every platform, whether you’re on Windows, Linux or Mac. \\

This software gives you the possibility to get different output representation. We choose the \href{https://github.com/amrisi/amr-guidelines/blob/master/amr.md}{amr representation} since it was the easiest one to understand and to implement.\\

We decided to build our own ROS wrapper, \href{https://github.com/WalkingMachine/lu4r_ros}{lu4r\_ros}  to better interface it with our task planning approach. We first translate the answer given by LU4R into simple format we call ActionForms. The ActionForms contains an action followed by all of its possible parameters as identified in the \href{https://framenet2.icsi.berkeley.edu/fnReports/data/luIndex.xml}{FrameNet Index of Lexical Units}
The ActionWrappers are then fed into a FIFO based priority manager and finally send to our task planner.\\

What is also interesting about LU4R, it’s that it will use the semantic mapping in it’s analysing process. All you have to do is to provide the correct pose for every object in the robot’s environment. You can also precise various synonym for every object to get a better understanding of inputted sentence.\\


\subsection{Object recognition}
\tab As a beginning team, we are still exploring various solution around the object recognition problem. Our first plan was to use the object recognition kitchen package from Willow Garage. But after using it in competition, we realised that the performance and the easiness to use wasn’t what the approach we were looking for. As an alternative, we start using the YOLO (You Only Look Once) ros package (link). \\

YOLO is a real-time object detection. It does not only detect various object but it also predict the bounding boxes of the detected object. It use a single neural network which is applied to the image. Multiple regions are then created and are used to predict the bounding boxes. Each bounding boxes also contain are predicted probability which are used to filter the predicted objects. The advantage of this system is that it can detect multiple objects in a real-time scenario.\\
 
\begin{figure}
  \centering
  \includegraphics[width=200pt]{images/frame_to_box.png}
  \caption{2D bounding box to 3D grasping pose, current technique}
\end{figure} 
 
We use the ROS package to make our job easier since this gave us the possibility to directly get the recognized objects output into a ROS topic. We can also get the bounding box for each detected object. First step we had to do was to transform those 2D bounding boxes in 3D to get a specific pose according to our robot. \\

For the moment, we created the package \href{https://github.com/WalkingMachine/wm_frame_to_box}{wm\_frame\_to\_box} to approximate the object pose by the depth point of the center of the bounding box. Even though it can have flaws, this technique has also proven to be largely sufficient for most of our applications and most importantly, it uses way less processing power than the full 3D pattern matching we used before. This allow us to do real time object positioning, a capability we are proud of.\\

Afterward, to get better results and a better pose, we plan to substract the point cloud according to the bounding boxes. We will then use it with the pointcloud segmentation from the \href{http://pointclouds.org}{PCL library} to extract the specific object and send it to a grasp identification package like \href{http://http://wiki.ros.org/haf_grasping}{haf\_grasping}. This technique, compare to what we are actually using would give us the possibility to grab a much wider variety of object.\\

\begin{figure}
  \centering
  \includegraphics[width=300pt]{images/frame_to_box2.png}
  \caption{2D bounding box to 3D grasping pose, future technique}
\end{figure}


For the moment, we are using the YOLO model but, we are looking forward to train our own dataset, just like we would do in competition. As an undergrad team, this is new for us but we have all the tools we need to overcome this. \\


\subsection{Navigation}
\tab In addition of last year navigation stack, this year we are using the pointcloud-to-lasercan package as a lighter solution for 3D navigation. This ensure a safe navigation around object like table or chair, since only the legs are detected by the lidar laser. Another great thing with this implementation is the fact that we can set the maximum height for collision avoidance. That way, if there’s an obstacle at head level, our robot will avoid it the same way it would avoir an object on the floor. \\

\subsection{Environmental reasoning}
\tab As novelty this year, we implemented our own solution for an environment representation in a way that we think is simple and easy for everyone. Our package named \href{http://github.com/walkingmachine/wonderland}{wonderland} is an agnostic system in the same way than LU4R. It’s composed of a server that received HTTP query based on a custom API. \\

The first thing that is needed when you start using this platform is to populate the database. This can be done manually if you already know the object position, but this can be done by your robot through POST query. It’s also possible to specify some room with a specific position relate to it. Once this is done, you can call our API and for example, just by doing a GET request on the url http://wonderland:8000/object, this will return you the list of every object the database contains. It’s also possible possible to filter the request by giving a known color link to the object or a location as a parameter. \\

Since the database is host as a server, you can access it from everywhere. You could decide to export it to another computer, run in cloud or just put it on the robot system itself. It also give the possibility to have a dynamic knowledge, meaning that the robot can update it’s knowledge of his environment in real-time. \\



\section{Conclusions and future work} 
\tab As you can see, despite being a bunch of undergraduate students, our team is about to catch up with the rest of the league. We’ve recently put a lot of efforts into stabilising our platform and fixing as much bugs as possible to give us a strong foot to move forward.
With it’s new swappable batteries system, object detection network and natural language processing, our robot has become a fully functional autonomous platform allowing us to focus our efforts on the challenges themselves instead of continuously fixing our hardware.
\\

\include{RobotDescription}
	

\section*{Team members}
Jeffrey Cousineau, Philippe La Madeleine, Maxime St-Pierre, Nathalie Connolly, Jimmy Poirier, Léonore Jean-François, Samuel Otis, Redouane Laref, Louis-Charle Labarre, Lucas Maurice, Nicolas Nadeau, Simon Landry, Cheuk Fai Shum, Veronica Romero Rosales, Nicolas Bernatchez, Quentin Gaillot, Raphael Duchaine, Jean-Frederic Boivin 

\nocite{*}
\bibliographystyle{plain}
\bibliography{references}


\end{document} 
