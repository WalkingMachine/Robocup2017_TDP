
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the TDPTemplate using
% the LaTeX document class 'llncs.cls' Springer LNAI format
% used in the RoboCup Symposium submissions.
% http://www.springer.com/computer/lncs?SGWID=0-164-6-793341-0
%
% It may be used as a template for your own TDP - copy it
% to a new file with a new name and use it as the basis
% for your Team Description Paper
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[runningheads,a4paper]{llncs}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{tabto}
\usepackage{lipsum}
\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage[T1]{fontenc}

\newcommand\notes[1]{\textcolor{red}{#1}}


\begin{document}


\newif\ifdraft
\draftfalse


\ifdraft
\setlength{\belowcaptionskip}{-5pt}
\fi

\title{Walking Machine @Home \newline \: 2019 Team Description Paper}

\author{Jeffrey Cousineau, Huynh-Anh Le, et al.}
\institute{École de Technologie Supérieure \\ 1100 rue Notre-Dame Ouest, Montreal, QC, Canada H3C 1K3 \\
\texttt{http://walkingmachine.ca,} \texttt{walking@ens.etsmtl.ca,} \texttt{https://github.com/WalkingMachine}}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

This paper gives details about the RoboCup@Home league team Walking Machine, from ETS University in Montreal, Canada for the next competition in Sydney, in July 2019. The robot from Walking Machine, named S.A.R.A. for "Systeme d'Assistance Robotique Autonome" (in English, Automated Robotic Assistance System), is a robot entirely built by the scientific club from ETS, mainly composed of undergraduates students. The robot is used for social interaction with humans, navigation and object manipulation. This document shows the electrical, mechanical and software novelties and functionalities of S.A.R.A.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\tab Walking Machine’s team is a young team from Montreal, Quebec, in Canada, composed of engineering students in the field of mechanical, electrical and software engineering. We have been working really hard to improve our robot for the next year Robocup@Home competition. As this would be our fourth participation, we learned a lot at Montreal Robocup and we made many improvements to get better results, mostly on the software side. In the past, the team went in many competitions like the Eurobot, but made the leap for the RoboCup@Home competition to get a bigger challenge and to get an opportunity to bring novelty in the scientific community surrounding robotic. \\


SARA, our creation, was designed for polyvalent human-robot interaction as well as efficient navigation and object manipulation. Our robot is mounted on four mecanum wheels powered by Roboteq drives, has one arm mimicking a normal human arm, and sensors for communication and navigation. Our team has developed knowledge in object and people detection/recognition, as well as navigation using a laser scanner, odometry on the wheels and a Asus Xtion camera. All of these parts are interfaced through ROS (Robot Operating System). \\

\section{Mechanical improvement}

\subsection{Mechanical}

\tab To improve our robot habilities, we decide to add a vertical linear actuator, more specifically, a TL5 column made by TiMOTION. This will add a degree of freedom, giving us a wider range of motion to reach objects on the floor or higher on the cupboard shelves. This will be really helpful for challenge like storing groceries where the objects can be anywhere in the cupboard. \\

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth]{images/robot_low.PNG}
    \caption{Lowest point}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth]{images/robot_high.PNG}
    \caption{Highest point}
  \end{subfigure}
  \caption{S.A.R.A. linear actuator motion range}
  \label{fig:coffee}
\end{figure}

We also decided to improve our wrist by adding a gearbox, giving it more strength. We took the decision to improve it after having problems with the gripper being too heavy.  

\newpage
\section{Software}

\subsection{Natural language understanding}
\tab To convert spoken data in actions subset, we had to create our own natural language understanding system (\href{http://github.com/walkingmachine/wm\_nlu}{http://github.com/walkingmachine/wm\_nlu}. To do this, we based ourself on rasa nlu\cite{rasa}, an open-source nlp tool for intent classification and entity extraction. But a simple entity extraction wasn't enough for us, we wanted a system that would take a command as an input and output the desired actions.\\

To do this, our first step was to create a dataset for the entity classification. Based on the GPSRCmdGen, we generated sentences which we hand labeled by attributing an entity to each specific type of sentence paired with specifics parameters.\\

We then built a ROS service which take a sentence as an input, classify the main intent using our dataset and return an array of actions that the robot need to execute according to the command. Our system is dependent of our environment reprensentation package (\href{http://github.com/walkingmachine/wonderland}{http://github.com/walkingmachine/wonderland}) since queries are made to our database.

\begin{figure}
  \centering
  \includegraphics[width=200pt]{images/wm_nlu.png}
  \caption{Natural language understanding process}
\end{figure} 

\subsection{Sound localization}
\tab

\subsection{Object recognition}
\tab For our object recognition, we use YOLO \cite{yolo}, a real-time object detection. It does not only detect various object but it also predicts the bounding boxes of the detected object. It uses a single neural network which is applied to the image. Multiple regions are then created and are used to predict the bounding boxes. Each of them also contains the predicted probability which is used to filter the predicted objects. The advantage of this system is that it can detect multiple objects in a real-time scenario.\\
 

\subsection{Objects and people tracking}

\begin{figure}
  \centering
  \includegraphics[width=200pt]{images/wm_data_collector.png}
  \caption{ wm\_data\_collector process}
\end{figure} 

\newpage

\section{Conclusions and future work} 
\notes{Pourrait être rephrasé.}
As you can see, despite being a group of undergraduate students, our team is about to catch up with the rest of the league. We’ve recently put a lot of efforts into stabilizing our platform and fixing as many bugs as possible to give us a strong foot to move forward.\\

\notes{Actualiser le dernier paragraphe.}
With its new swappable batteries system, object detection network and natural language processing, our robot has become a fully functional autonomous platform allowing us to focus our efforts on the challenges themselves instead of continuously fixing our hardware.
\\

\include{RobotDescription}
	
\section*{Team members}
André-Philippe Audette,
Nicolas Bernatchez,
Pierre-Emmanuel Billeau,
Jeffrey Cousineau, 
Raphael Duchaine,
Quentin Gaillot,
Louis-Charle Labarre, 
Philippe La Madeleine,  
Redouane Laref,
Vincent Lavoie-Marchildon,
Huynh-Anh Le,
Lucas Maurice,
Alexandre Mongrain,
Jimmy Poirier,
Veronica Romero Rosales

\nocite{*}
\bibliographystyle{plain}
\bibliography{references}


\end{document} 
