
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the TDPTemplate using
% the LaTeX document class 'llncs.cls' Springer LNAI format
% used in the RoboCup Symposium submissions.
% http://www.springer.com/computer/lncs?SGWID=0-164-6-793341-0
%
% It may be used as a template for your own TDP - copy it
% to a new file with a new name and use it as the basis
% for your Team Description Paper
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[runningheads,a4paper]{llncs}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{tabto}
\usepackage{lipsum}
\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage[T1]{fontenc}

\newcommand\notes[1]{\textcolor{red}{#1}}


\begin{document}


\newif\ifdraft
\draftfalse


\ifdraft
\setlength{\belowcaptionskip}{-5pt}
\fi

\title{Walking Machine @Home \newline \: 2019 Team Description Paper}

\author{Jeffrey Cousineau, Huynh-Anh Le, et al.}
\institute{École de Technologie Supérieure \\ 1100 rue Notre-Dame Ouest, Montreal, QC, Canada H3C 1K3 \\
\texttt{http://walkingmachine.ca,} \texttt{walking@ens.etsmtl.ca,} \texttt{https://github.com/WalkingMachine}}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

This paper gives details about the RoboCup@Home league team Walking Machine, from ETS University in Montreal, Canada for the next competition in Sydney, in July 2019. The robot from Walking Machine, named S.A.R.A. for "Systeme d'Assistance Robotique Autonome" (in English, Automated Robotic Assistance System), is a robot entirely built by the scientific club from ETS, mainly composed of undergraduates students. The robot is used for social interaction with humans, navigation and object manipulation. This document shows the electrical, mechanical and software novelties and functionalities of S.A.R.A.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\tab Walking Machine’s team is a young team from Montreal, Quebec, in Canada, composed of engineering students in the field of mechanical, electrical and software engineering. We have been working really hard to improve our robot for the next year Robocup@Home competition. As this would be our fourth participation, we learned a lot at Montreal Robocup and we made many improvements to get better results, mostly on the software side. In the past, the team went in many competitions like the Eurobot, but made the leap for the RoboCup@Home competition to get a bigger challenge and to get an opportunity to bring novelty in the scientific community surrounding robotic. \\


SARA, our creation, was designed for polyvalent human-robot interaction as well as efficient navigation and object manipulation. Our robot is mounted on four mecanum wheels powered by Roboteq drives, has one arm mimicking a normal human arm, and sensors for communication and navigation. Our team has developed knowledge in object and people detection/recognition, as well as navigation using a laser scanner, odometry on the wheels and a Asus Xtion camera. All of these parts are interfaced through ROS (Robot Operating System). \\

In the rest of this paper we will present in the second section the mechanical improvements we've made to our robot to overcome the differents challenge. In section 3, the different packages we've develop are described. And finally in section 4 this paper will conclude and explore the expected features for next year Robocup. \\

\section{Mechanical improvement}

\tab To improve our robot habilities, we decide to add a vertical linear actuator, more specifically, a TL5 column made by TiMOTION. This will add a degree of freedom, giving us a wider range of motion to reach objects on the floor or higher on the cupboard shelves. This will be really helpful for challenge like storing groceries where the objects can be anywhere in the cupboard. \\

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.15\linewidth}
    \includegraphics[width=\linewidth]{images/sara_render_retracted.png}
    \caption{Robot retracted}
  \end{subfigure}
  \hspace{1.5cm}
  \begin{subfigure}[b]{0.15\linewidth}
    \includegraphics[width=\linewidth]{images/sara_render_extracted.png}
    \caption{Robot extracted}
  \end{subfigure}
  \caption{S.A.R.A. linear actuator motion range}
  \label{fig:coffee}
\end{figure}

We also decided to improve our wrist by adding a gearbox, giving it more strength. We took the decision to improve it after having problems with the gripper being too heavy.  

\begin{figure}
  \centering
  \includegraphics[width=125pt]{images/wrist.png}
  \caption{Improved wrist 3D render}
\end{figure} 

\newpage
\section{Software}

\subsection{Natural language understanding}
\tab To convert spoken data in actions subset, we had to create our own natural language understanding system (\href{http://github.com/walkingmachine/wm\_nlu}{http://github.com/walkingmachine/wm\_nlu}. To do this, we based ourself on rasa nlu\cite{rasa}, an open-source nlp tool for intent classification and entity extraction. But a simple entity extraction wasn't enough for us, we wanted a system that would take a command as an input and output the desired actions.\\

To do this, our first step was to create a dataset for the entity classification. Based on the GPSRCmdGen, we generated sentences which we hand labeled by attributing an entity to each specific type of sentence paired with specifics parameters.\\

We then built a ROS service which take a sentence as an input, classify the main intent using our dataset and return an array of actions that the robot need to execute according to the command. Our system is dependent of our environment reprensentation package (\href{http://github.com/walkingmachine/wonderland}{http://github.com/walkingmachine/wonderland}) since queries are made to our database.

\begin{figure}
  \centering
  \includegraphics[width=300pt]{images/wm_nlu.png}
  \caption{Natural language understanding process}
\end{figure} 

\subsection{Sound localization}
\tab To improve our performances mainly in the SPR challenge and to add reactivity to our robot, we decided to add a Matrix Creator which include a microphone array coupled with a Raspberry Pi 3. We decided to use ODAS  \cite{ODAS} which stands for Open embeddeD Audition System. This is a library dedicated to perform sound source localization, tracking, separation and post-filtering, developped by IntRoLab\cite{Introlab} from Sherbrooke university in Quebec.\\ 

At first, a simple sound source localization is used. But then a Kalman filter is applied to perform sound source tracking. It help us eliminated simple noise to tracking a person talking to the robot. We can even go further with this library by using the sound source separation which help us separate the sound incoming form different speaker.\\

We decided to build our own ROS wrapper around ODAS considering the lack of documentation surrounding the project. Our wrapper offer multiple topics which publish either the differents sound sources localization, the tracked sound source or the separated sound sources. We can then easily identify the location of a speaker giving a command to our robot.\\

\subsection{Object recognition}
\subsubsection{Recognition system}
\hfill\\

For our object recognition, we use YOLO \cite{yolo}, a real-time object detection. It does not only detect various object but it also predicts the bounding boxes of the detected object. It uses a single neural network which is applied to the image. Multiple regions are then created and are used to predict the bounding boxes. Each of them also contains the predicted probability which is used to filter the predicted objects. The advantage of this system is that it can detect multiple objects in a real-time scenario.\\

\subsubsection{Dataset creation tool}
\hfill\\

This year we are putting our efforts on a way to simplify the dataset creation. During the last Robocup in Montreal, it was the first time our team had an efficient object recognition system. But our flaw was in the production of our dataset. Since we are retraining over ImageNet pre-trained weight, we need to provide a large dataset and for this we had to do the all the bounding boxes by hand for every images.\\
 
We decided that we needed to find a faster way to train the provided objects from the arena. Our plan is now to use a rotating platform with a greenscreen, that way we could automate the data collection process by using background substraction technique with OpenCV and contour detection to find the object bounding box. Using the substracted object, we can now apply different transformations to do dataset augmentation. \\

For the moment, we only created the software part to automated the bounding boxes. As you can see, on the Fig.4 a), we applied an InRange filter, on the Fig.4 b), we inverted the image to finally apply the findcontour function on Fig.4 c). By taking the largest contour found, we can easily calculate the bouding box of the object. \\
 
\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/bounding_inrange.png}
    \caption{InRange filter}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/bounding_invert.png}
    \caption{Invert image}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/bounding_contours.png}
    \caption{Contour detection}
  \end{subfigure}
  \caption{Dataset creation process with wm\_dataset\_preparation}
  \label{fig:coffee}
\end{figure}  
 

\subsection{Objects and people tracking}

\begin{figure}
  \centering
  \includegraphics[width=300pt]{images/wm_data_collector.png}
  \caption{ wm\_data\_collector process}
\end{figure} 

\newpage

\section{Conclusions and future work} 
\tab In this paper, we presented how we are developping our own robotic platform for the Robocup@Home competition. 

\include{RobotDescription}
	
\section*{Team members}
André-Philippe Audette,
Nicolas Bernatchez,
Pierre-Emmanuel Billeau,
Jeffrey Cousineau, 
Raphael Duchaine,
Quentin Gaillot,
Louis-Charle Labarre, 
Philippe La Madeleine,  
Redouane Laref,
Vincent Lavoie-Marchildon,
Huynh-Anh Le,
Lucas Maurice,
Alexandre Mongrain,
Jimmy Poirier,
Veronica Romero Rosales

\nocite{*}
\bibliographystyle{plain}
\bibliography{references}


\end{document} 
